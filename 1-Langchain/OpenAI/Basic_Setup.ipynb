{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f92217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Set API keys for OpenAI and LangSmith\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "##langsmith tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e3b9db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x7399c81d3220> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7399a2d78ee0> root_client=<openai.OpenAI object at 0x7399c81d3670> root_async_client=<openai.AsyncOpenAI object at 0x7399a2c55120> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "#Initialize LLM Model (OpenAI)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create LLM instance using GPT-4o\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4671bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input and get response from LLM\n",
    "result = llm.invoke(\"What is Generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd76a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI refers to a category of artificial intelligence systems designed to generate new content, data, or solutions by learning patterns from existing data. These systems are capable of creating text, images, music, and other types of content that resemble human-created work. Unlike traditional AI models that perform classification or prediction tasks, generative AI models focus on producing novel outputs.\\n\\nSome key concepts and technologies associated with generative AI include:\\n\\n1. **Generative Adversarial Networks (GANs):** A type of neural network architecture composed of two competing networks—a generator and a discriminator—that work together to produce increasingly realistic data samples.\\n\\n2. **Variational Autoencoders (VAEs):** A type of autoencoder that learns a probabilistic representation of data and is used to generate new data samples by sampling from this learned distribution.\\n\\n3. **Transformer Models:** Originally designed for natural language processing tasks, transformers have become a foundational technology in generative AI, with models like GPT (Generative Pre-trained Transformer) providing powerful capabilities for text generation.\\n\\n4. **Diffusion Models:** These models are used to generate data through a process that iteratively transforms simple probability distributions into complex ones.\\n\\nGenerative AI has numerous applications, including content creation, data augmentation, drug discovery, and more. It is also playing a significant role in advancing fields such as virtual reality and entertainment by enabling the creation of immersive and realistic digital experiences. However, it also raises ethical concerns and challenges, such as the potential for creating misleading information and the need for responsible use and regulation.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 310, 'prompt_tokens': 12, 'total_tokens': 322, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': None, 'id': 'chatcmpl-Bv0loORRkOB51CWvO0G6v5Az1pWml', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e6c86b90-1ae0-4db2-9cf6-7b9930ceffa8-0' usage_metadata={'input_tokens': 12, 'output_tokens': 310, 'total_tokens': 322, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c730f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are an expert AI Engineer. Provide me answer based on the question '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='please ask the question: {input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "### prompt template: how you want your llm to behave or what kind of role you want to assign\n",
    "#chatprompt template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are an expert AI Engineer. Provide me answer based on the question \"),\n",
    "        (\"user\",\"please ask the question: {input}\")\n",
    "    ]\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e285822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are an expert AI Engineer. Provide me answer based on the question '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='please ask the question: {input}'), additional_kwargs={})]) middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7399c81d3220>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7399a2d78ee0>, root_client=<openai.OpenAI object at 0x7399c81d3670>, root_async_client=<openai.AsyncOpenAI object at 0x7399a2c55120>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n"
     ]
    }
   ],
   "source": [
    "# Combine prompt and LLM into a runnable chain\n",
    "\n",
    "chain = prompt|llm\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40759d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a specific input through the prompt+LLM chain\n",
    "response = chain.invoke({\"input\":\"can you tell me about langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca3fb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langchain is a framework designed to facilitate the development of applications powered by large language models (LLMs). By providing a comprehensive suite of tools, Langchain allows developers to create, integrate, and manage LLMs in a wide array of use cases seamlessly. This includes capabilities for memory management, prompt engineering, querying APIs, and more, making it easier to build sophisticated applications that leverage the natural language understanding and generation capabilities of LLMs. As of my knowledge cutoff in October 2023, Langchain emphasizes modularity and extensibility, helping developers focus more on innovation and less on the complexities of working directly with LLMs.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 39, 'total_tokens': 167, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bv0lu5TQ1Gfo1ma6dWC6wXhxqyjHp', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--06757c09-99a1-40bf-9252-ec0fc8b37a24-0', usage_metadata={'input_tokens': 39, 'output_tokens': 128, 'total_tokens': 167, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
